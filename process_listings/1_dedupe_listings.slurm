#!/bin/bash

# spark job

#SBATCH --job-name=process_listings
#SBATCH --output=job-%j.out
#SBATCH --error=job-%j.err

#SBATCH --time=02:00:00
#SBATCH --ntasks=18
#SBATCH --cpus-per-task=2
#SBATCH --mem-per-cpu=1G
#SBATCH --constraint=skylake

#SBATCH --account=p_dsi
#SBATCH --partition=production
#SBATCH --mail-user=shea.conaway@vanderbilt.edu
#SBATCH --mail-type=ALL
#SBATCH --export=ALL

# load modules
module purge
module load GCCcore/.8.2.0 Spark/2.4.0

date
grep "#SBATCH" process_listings.slurm

# job script
export APP="process_listings.py $SLURM_JOB_ID"

# scratch dir for temp job info
export JOB_ID=$SLURM_JOB_ID
export JOB_HOME="/scratch/$USER/$JOB_ID"
echo "JOB_HOME=$JOB_HOME"
mkdir -p $JOB_HOME 

# calc number of workers
export NWORKERS=$(( $SLURM_NTASKS - 2 ))

# id of the last process
export LAST_PROC=$(( $SLURM_NTASKS - 1 ))

# spark environment
export SPARK_HOME=/accre/arch/easybuild/software/Compiler/GCCcore/8.2.0/Spark/2.4.0
export SPARK_MASTER_PORT=7077
export SPARK_MASTER_WEBUI_PORT=8080
export SPARK_DAEMON_MEMORY=1g
export SPARK_WORKER_CORES=$SLURM_CPUS_PER_TASK
export SPARK_WORKER_MEMORY=$(( SLURM_MEM_PER_CPU * $SLURM_CPUS_PER_TASK - 1000))m
export SPARK_EXECUTOR_CORES=$SLURM_CPUS_PER_TASK
export SPARK_EXECUTOR_MEMORY=$SPARK_WORKER_MEMORY

# execute accre spark configs
source "$SPARK_HOME/sbin/spark-config.sh"
source "$SPARK_HOME/bin/load-spark-env.sh"

# creat config file for job
cluster_conf=$"# This file has been generated by $0\n"
cluster_conf+=$"0         ./task-roles.sh CLIENT\n"
cluster_conf+=$"1         ./task-roles.sh MASTER\n"
cluster_conf+=$"2-$LAST_PROC      ./task-roles.sh WORKER"
echo -e $cluster_conf > cluster.conf

# run job
# note: errors in stdout. no need for err files
srun -l -o task%t-%j.out --multi-prog cluster.conf



